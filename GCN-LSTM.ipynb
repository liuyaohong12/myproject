{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T02:01:39.110526200Z",
     "start_time": "2024-05-30T02:01:37.214888900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "import torch\n",
    "import dgl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from dgl.nn import GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T02:01:40.259671600Z",
     "start_time": "2024-05-30T02:01:40.251520100Z"
    }
   },
   "outputs": [],
   "source": [
    "class processing(nn.Module):  #使用不同的方式来处理数据，普通lstm使用Linear,这里使用GCN。\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fn = GraphConv(input_size,hidden_size) #使用GCN\n",
    "    def forward(self, g, feat):   #feat:(nodes_num,feature_len)\n",
    "        feat = self.fn(g, feat)\n",
    "        feat = F.elu(feat)     #ELU激活函数，可以选择不使用\n",
    "        return feat\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.forget_gate = processing(input_size + hidden_size, hidden_size)\n",
    "        self.input_gate = processing(input_size + hidden_size, hidden_size)\n",
    "        self.output_gate = processing(input_size + hidden_size, hidden_size)\n",
    "        self.cell_gate = processing(input_size + hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self,g, feature, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "        combined = torch.cat((feature, h_prev), dim=1)\n",
    "        \n",
    "        f_t = torch.sigmoid(self.forget_gate(g,combined))\n",
    "        i_t = torch.sigmoid(self.input_gate(g,combined))\n",
    "        o_t = torch.sigmoid(self.output_gate(g,combined))\n",
    "        c_tilde = torch.tanh(self.cell_gate(g,combined))\n",
    "        \n",
    "        c_t = f_t * c_prev + i_t * c_tilde\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm_cells = nn.ModuleList([LSTMCell(input_size, hidden_size) if i == 0 else LSTMCell(hidden_size, hidden_size) for i in range(num_layers)])\n",
    "    \n",
    "    def forward(self, g,feature,state):\n",
    "        batch_size, seq_length, _ = feature.size()\n",
    "        if not state :                 #初始化h,c状态\n",
    "            h = [torch.zeros(batch_size, self.hidden_size).to(feature.device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(batch_size, self.hidden_size).to(feature.device) for _ in range(self.num_layers)]\n",
    "        else:\n",
    "            h,c = state\n",
    "        \n",
    "        #outputs = []\n",
    "        for t in range(seq_length):\n",
    "            input_t = feature[:, t, :]\n",
    "            for layer in range(self.num_layers):\n",
    "                h[layer], c[layer] = self.lstm_cells[layer](g,input_t, (h[layer], c[layer]))\n",
    "                input_t = h[layer]\n",
    "            #outputs.append(h[-1])  #这里我们只需要使用最后的h作为结果，如果需要用到所有结果再进行处理，则需要这样存储起来\n",
    "        \n",
    "        #outputs = torch.stack(outputs, dim=1)\n",
    "        return h[-1], (h, c)\n",
    "class GCN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_classes,num_layers):\n",
    "        super().__init__()\n",
    "        self.Encoder = LSTM(input_size,hidden_size,num_layers) \n",
    "        self.Decoder = nn.Linear(hidden_size,num_classes)\n",
    "    def forward(self, g, feat,state):   #feat:(nodes_num,feature_len)\n",
    "        h,state = self.Encoder(g,feat,state)\n",
    "        h = F.elu(h)     #ELU激活函数，可以选择不使用\n",
    "        h = self.Decoder(h)\n",
    "        return h,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T02:01:42.924673800Z",
     "start_time": "2024-05-30T02:01:42.534146Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_size = 12\n",
    "input_size = 4\n",
    "seq_length = 12\n",
    "hidden_size = 12\n",
    "num_classes = 1\n",
    "num_layers = 1\n",
    "nodes_num = 1440\n",
    "def generate_data(graph_size,nodes_num,seq_length,input_size,num_classes):\n",
    "    nodes_X = np.random.rand(graph_size,nodes_num, seq_length, input_size)\n",
    "    nodes_y = np.random.rand(graph_size,nodes_num, num_classes)\n",
    "    return nodes_X, nodes_y\n",
    "nodes_X, nodes_y = generate_data(graph_size,nodes_num,seq_length,input_size,num_classes)\n",
    "#生成模拟数据\n",
    "X_train = torch.tensor(nodes_X, dtype=torch.float32)\n",
    "y_train = torch.tensor(nodes_y, dtype=torch.float32) \n",
    "#生成边，这里我随机生成的，可以直接根据自己的需要来生成边\n",
    "# 设置边的生成概率\n",
    "edge_prob = 0.05\n",
    "# 构建基于概率的随机边\n",
    "src, dst = [], []\n",
    "for i in range(nodes_num):\n",
    "    for j in range(nodes_num):\n",
    "        if np.random.rand() < edge_prob:\n",
    "            src.append(i)\n",
    "            dst.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1440, 12, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1440, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T02:01:43.887154100Z",
     "start_time": "2024-05-30T02:01:43.725710700Z"
    }
   },
   "outputs": [],
   "source": [
    "graphs_train = []\n",
    "for X, Y in zip(X_train, y_train):\n",
    "    g = dgl.graph((src,dst))  # 这里只是示例，您可以根据实际情况构建不同的图\n",
    "    g.ndata['label'] = Y  # 将标签存储在节点特征字典中\n",
    "    g.ndata['feature'] = X\n",
    "    graphs_train.append(g)  #将所有的图存在一个图列表里，然后使用dataloader加载数据\n",
    "loader_train = GraphDataLoader(graphs_train, batch_size=4, shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T02:01:59.380654Z",
     "start_time": "2024-05-30T02:01:57.029675400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] , train-Loss: 0.35526806116104126\n",
      "Epoch [2/3] , train-Loss: 0.33588626980781555\n",
      "Epoch [3/3] , train-Loss: 0.31624722480773926\n"
     ]
    }
   ],
   "source": [
    "model = GCN_LSTM(input_size,hidden_size,num_classes,2).to(device)\n",
    "state=[]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss() # 或者根据你的任务选择其他损失函数\n",
    "for epoch in range(3):  # 假设训练3个epoch\n",
    "    model.train()\n",
    "    for batch in loader_train:\n",
    "        for S in state:\n",
    "            for s in S:\n",
    "                s.detach_()      #这里需要注意，我们在使用LSTM进行计算时，通常需要使用这个来将之前的state使用detach避免计算多余的梯度\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out,state = model(batch,batch.ndata['feature'],state)\n",
    "        loss = criterion(out, batch.ndata['label'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/3] , train-Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
